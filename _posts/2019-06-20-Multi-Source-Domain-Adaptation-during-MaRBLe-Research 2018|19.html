---
layout: post
excerpt: Maastricht University - Faculty of Psychology and Neuroscience (FPN)
permalink: /Multi-Source Domain Adaptation - MaRBLe Research 2018/19
published: true
images:
  - url: /assets/1_representation_network.png
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">
<img src="/assets/1_representation_network.png" /></p>
<h2><u>Introduction </u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Humans are able to recognize scenes independently of the modality they perceive it in. In this paper, it is tested if scene specific features extracted from natural images are reusable for classifying clip art and sketches. Further, the ability of networks to hold multiple representations simultaneously is examined. To study this problem a pretrained convolutional neural network is used. Further, a randomly initialized classifier is added and retrained. The representations of clip art achieve the highest results, followed by sketches. Natural image performance remains notably below prior results. The experiment suggests that the overall transferability of learned features is not only limited by the distance but also by the diversity of the input distribution. Moreover, a multimodal representation is shown to be feasible but only poorly. Further research is needed to explain the obtained outcomes and to explain why they deviate from previous expectations.
  
<h2><u>Can you recognize scenes across different styles? </u></h2>


<p><em>The  input  variables  consist  of  natural  images  from  theScene205  dataset  collected  and  curated  by  B.  Zhou  et  al.(2014).   An  exhaustive  list  of  the  classes  can  be  found  inappendix 1. The set contains around 2.5 million images from205 scene categories.  The compressed file contains resized 256*256  images,  split  into  a  train  set  and  a  validation  setof Places 205 with a size of 126GB. Due to computationallimitations the number of pictures is reduced to around 80pictures  per  object,  so  that  16,400  pictures  remain.    Thesketch images consist of 14,830 training and 2,050 valida-tion  sketches  collected  through  Amazon  Mechanical  Turk,whereby different colors indicate different objects.  The clipart  data  includes  11,372  training  and  1,954  validation  clipart images downloaded from search engines.  Sketches andclip  art  were  assembled  by  Aytar  et  al.  (2018). </em></p>
<p><img src="/assets/1_Overview-size1.png" /></p>
<p>And yes, while I&rsquo;m sure we&rsquo;ve all learned to treat statements like &ldquo;I used a deep neural network to solve X&rdquo; with some scrutiny, there&rsquo;s no denying that the advent of deep learning has fundamentally changed the way we approach creating intelligent systems.</p>
<p>So what is it about deep learning that has made it the centerpiece of all tech buzzwords? In my opinion, it comes down to the ability for deep learning models to perform analysis of forms of data such as <span style="color: #0000ff;">speech</span>, <span style="color: #0000ff;">text</span>, and <span style="color: #0000ff;">images</span> through <span style="color: #ff0000;">large labeled datasets</span>, <span style="color: #ff0000;">enormous compute power</span>, and <span style="color: #ff0000;">effective network architectures</span>.</p>
<p><img src="/assets/5Years2.png" /></p>

<h2><u>Natural Language Processing</u></h2>
<p><img src="/assets/5Years6.png" /></p>
<p>When we talk about natural language processing, there are a lot of different tasks that fit under this umbrella. Question answering, machine translation, sentiment analysis, document summarization, the list goes on and on. NLP is a large and broad field that encompasses advancements in linguistics and traditional AI approaches. The application of deep learning to NLP tasks has quite a bit of success, but we also have quite some ways to go.</p>
<p><strong>Impact on Industry</strong></p>
<ul>
<li><strong><font size="+2" color="#5792EA">G</font></strong><strong><font size="+2" color="#CB4B3F">o</font></strong><strong><font size="+2" color="#FAC604">o</font></strong><strong><font size="+2" color="#5792EA">g</font></strong><strong><font size="+2" color="#3DAB59">l</font></strong><strong><font size="+2" color="#CB4B3F">e</font></strong> and <strong><font size="+2" color="##3B5999">Facebook</font></strong> &ndash; Improvements in the field of machine translation, creating systems that can translate text into other languages, have been one of the biggest success stories in DL applied to NLP. Google and Facebook have both used an approach called Neural Machine Translation to <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html" target="_blank" rel="noopener">improve Google Translate</a> and to <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html" target="_blank" rel="noopener">create accurate translations of Facebook posts</a>.</li>
<li><strong><font size="+2" color="#E60302">Bai</font></strong><strong><font size="+2" color="#2319DC">du</font></strong>, <strong><font size="+2" color="#5792EA">G</font></strong><strong><font size="+2" color="#CB4B3F">o</font></strong><strong><font size="+2" color="#FAC604">o</font></strong><strong><font size="+2" color="#5792EA">g</font></strong><strong><font size="+2" color="#3DAB59">l</font></strong><strong><font size="+2" color="#CB4B3F">e</font></strong>, <strong><font size="+2">Apple</font></strong>, and <strong><font size="+2" color="#FE9800">Amazon</font></strong> - The rise of conversational agents like Siri, Alexa, Cortana, and Google Assistant can be attributed to more advanced speech recognition techniques that involve deep nets. We've seen press releases and blog posts from <a href="http://research.baidu.com/deep-speech-3%EF%BC%9Aexploring-neural-transducers-end-end-speech-recognition/" target="_blank" rel="noopener">Baidu</a>, <a href="https://9to5google.com/2017/06/01/google-speech-recognition-humans/" target="_blank" rel="noopener">Google</a>, <a href="https://machinelearning.apple.com/2017/08/06/siri-voices.html" target="_blank" rel="noopener">Apple</a>, and <a href="https://www.slideshare.net/AmazonWebServices/aws-reinvent-2016-deep-learning-in-alexa-mac202" target="_blank" rel="noopener">Amazon</a>. These companies all definitely have their own unique twists on their models, but the general idea of using systems with RNNs, LSTMs, Seq2Seq models, and/or CTC loss functions is applicable across the board.</li>
<li><strong><font size="+2" color="#08A0E9">Twitter</font></strong> &ndash; With about 350 million tweets sent per day, Twitter definitely has a lot of textual information to parse through. One of their recent <a href="https://blog.twitter.com/engineering/en_us/topics/insights/2017/using-deep-learning-at-scale-in-twitters-timelines.html" target="_blank" rel="noopener">blog posts</a> discusses a custom neural net that determines the ranking of a set of tweets to show on a user&rsquo;s feed.</li>
<li><strong><font size="+2" color="#BE2620">Quora</font></strong> &ndash; Earlier this year, Quora released a <a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" target="_blank" rel="noopener">dataset</a> that contained question pairs, as well as labels for whether or not the two sentences in each pair are duplicates of each other. While it&rsquo;s unclear what type of system Quora uses behind the scenes to address the duplicate question problem, some sort of ML (or DL) might currently be used.</li>
<li><strong><font size="+2" color="#19DA5B">Spotify</font></strong> &ndash; Typically used for image processing tasks, CNNs were the DL method of choice for <a href="http://benanne.github.io/2014/08/05/spotify-cnns.html" target="_blank" rel="noopener">Spotify&rsquo;s music recommendation system</a> (This post is a bit old by DL standards &ndash; 2014 &ndash; so it&rsquo;d be interesting to find out what updates they&rsquo;ve made). This system was used to augment the collaborative filtering algorithm that has traditionally been used.</li>
<li><strong><font size="+2" color="#159DD5">Salesforce</font></strong> &ndash; In 2016, <a href="https://techcrunch.com/2016/04/04/saleforce-acquires-metamind/" target="_blank" rel="noopener">Salesforce acquired MetaMind</a>, a startup headed by Stanford professor Richard Socher. Since then, the group has primarily been doing NLP research, helping set the stage for <a href="https://www.salesforce.com/products/einstein/overview/" target="_blank" rel="noopener">Einstein</a>, one of Salesforce&rsquo;s main products. One interesting feature is their <a href="https://www.salesforce.com/blog/2017/05/ai-salesforce-research-text-summarization.html" target="_blank" rel="noopener">text summarization</a> capability that uses both encoder/decoder RNNs as well as reinforcement learning to summarize text articles. The group also has an active <a href="https://einstein.ai/research" target="_blank" rel="noopener">blog</a> that is more technically oriented.</li>
</ul>
<p><strong>Important Papers</strong></p>
<ul>
<li><a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>Word2Vec</strong></span></a> (2013) - First effective and scalable method of generating dense word vectors from a text corpora.</li>
<li><a href="https://arxiv.org/pdf/1409.3215.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>Seq2Seq</strong></span></a> (2014) - Deep neural net that maps sequences to other sequences. A very general approach that has served as a base for a lot of future breakthroughs in DL applied to NLP. One of the biggest subsequent breakthroughs was the attention mechanism, introduced in <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" rel="noopener">this paper</a>.</li>
<li><span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1412.5567.pdf" target="_blank" rel="noopener">Deep Speech</a></strong></span> (2014) &ndash; Work from authors at Baidu that illustrated the first scalable use of an end-to-end deep learning network architecture for speech recognition. See <span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1512.02595.pdf" target="_blank" rel="noopener">Deep Speech 2</a></strong></span> (2015), <span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1508.04395.pdf" target="_blank" rel="noopener">Attention-Based SR</a></strong></span> (2015), and <span style="text-decoration: underline;"><strong><a href="http://research.baidu.com/deep-speech-3%EF%BC%9Aexploring-neural-transducers-end-end-speech-recognition/" target="_blank" rel="noopener">Deep Speech 3</a></strong></span> (2017) for advancements that largely stemmed from this paper.</li>
<li><span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1506.07285.pdf" target="_blank" rel="noopener">Dynamic Memory Networks for NLP</a></strong></span> (2015) &ndash; Applied a neural network architecture to the task of question answering.</li>
<li><span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1609.08144v2.pdf" target="_blank" rel="noopener">Neural Machine Translation</a></strong></span> (2016) - Google's paper describing their approach to translating text from one language to another.</li>
</ul>
<h2><u>Reinforcement Learning </u></h2>
<p><img src="/assets/5Years7.png" /></p>
<p>RL approaches are most commonly seen in the robotics fields, where I think the Holy Grail amounts to creating an agent or robot that is able to learn how to perform any task we want. There have been great advances, evidenced by agents that can beat Atari games and Go, but I think it&rsquo;ll still be a couple years before we really see an impact on products and services that we use every day.</p>
<p><strong>Impact on Industry</strong></p>
<ul>
<li><strong><font size="+2" color="#7B7C7E">Deepmind</font></strong> &ndash; Similar to OpenAI, DeepMind isn&rsquo;t a company in the traditional sense, but they&rsquo;ve been the most instrumental in pushing RL research. From inventing DQN to creating the famous AlphaGo system, DeepMind has been a key contributor to deep RL research. You can learn more about what they&rsquo;re currently working on through their <a href="https://deepmind.com/blog/differentiable-neural-computers/">blog</a>.</li>
<li><strong><font size="+2" color="#7BBAD8">Boston</font></strong><strong><font size="+2" color="#062238"> Dynamics</font></strong> &ndash; While it's unclear <a href="https://www.quora.com/What-kind-of-learning-algorithms-are-used-on-Boston-Dynamics-robots" target="_blank" rel="noopener">how much deep learning is used in their systems</a> (they seem to do a lot more feature engineering), they are a company that has made a lot of cool advances and is definitely one to keep an eye on. And honestly, I had to include them because <a href="https://www.youtube.com/watch?v=fRj34o4hN4I">this is just too cool</a>.</li>
<li><strong><font size="+2" color="#552961">OpenAI</font></strong> &ndash; While it&rsquo;s hard to even say whether OpenAI is really part of industry or if they&rsquo;re just a really well funded research lab, it&rsquo;s clear that in trying to achieve their mission to &ldquo;build safe AGI&rdquo;, the road involves research in RL. They frequently update their <a href="https://blog.openai.com/">blog</a> where they talk about new advancements in <a href="https://blog.openai.com/competitive-self-play/">self-play</a>, <a href="https://blog.openai.com/openai-baselines-ppo/">policy optimization</a>, and <a href="https://blog.openai.com/learning-to-cooperate-compete-and-communicate/">multiagent cooperation</a>.</li>
<li><strong><font size="+2" color="#4AA73C">Bons</font></strong><strong><font size="+2" color="#77C539">ai</font></strong> &ndash; Bonsai is a Berkeley based startup that wants to abstract away the complexity of building AI systems by creating a platform that businesses can use to create and deploy ML models (with a focus on RL). They also have an active <a href="https://bons.ai/blog">blog</a> that has interesting pieces on RL, industrial AI, and interpretability.</li>
</ul>
<p>The next &lsquo;Important Papers&rsquo; section is interesting to me because RL powered by deep learning is still in its infancy. Reinforcement learning, in general, is difficult. Getting an agent to do what you want it to do in an unknown environment with continuous state and action spaces is not a trivial task by any stretch. While the advancements shown through Atari games and AlphaGo are fantastic breakthroughs, it&rsquo;s been difficult to see how much of what we&rsquo;ve learned through current advancements can be transferred to tasks that can be useful in industry.</p>
<p>So, why is this transfer difficult? Well, this is in part because of the way that board games and computer games are structured. In Atari games and Go, the agent is making decisions and actions in a space where the environment is deterministic. We know exactly how the board state will change when the agent decides to place a white stone on row 20, column 13. With a lot of RL tasks in the real world, the environments are a little more difficult. The action space and state space for the agent can be continuous and there is almost an unlimited amount of noise and variability that the agent will encounter. For those interested, check out Andrej Karpathy&rsquo;s <a href="https://medium.com/@karpathy/alphago-in-context-c47718cb95a5">blog post</a> on this distinction.</p>
<p>The difficulties with handling partially observed and indeterministic environments, with continuous action and state spaces makes RL a problem that even DL methods have had trouble with. It&rsquo;ll be interesting to see how the field progresses. Here are a couple of past advancements.</p>
<p><strong>Important Papers</strong></p>
<p>Check out this <a href="https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-2-Reinforcement-Learning" target="_blank" rel="noopener">blog post</a> if you&rsquo;re interested in more in depth overviews.</p>
<ul>
<li><a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>Atari with DQN</strong></span></a> (2013) and subsequent <span style="text-decoration: underline;"><strong><a href="https://www.nature.com/articles/nature14236" target="_blank" rel="noopener">Nature Paper</a></strong></span> (2015) &ndash; First successful use of deep learning in RL. Introduced DQN (Deep Q-Network), which is an end to end RL agent that uses a large neural net to process game states and choose appropriate actions.</li>
<li><a href="https://arxiv.org/pdf/1602.01783v2.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>Asynchronous Methods for Deep RL</strong></span></a> (2016) &ndash; Introduced the A3C algorithm that expanded and improved upon on DQN.</li>
<li><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>AlphaGo</strong></span></a> (2016) &ndash; Described the approach used to create the AlphaGo system that <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol" target="_blank" rel="noopener">beat Lee Sedol in the summer of 2016</a>. Monte Carlo Tree Search and DNNs were the primary components in the system.</li>
<li><span style="text-decoration: underline;"><strong><a href="https://www.nature.com/articles/nature24270" target="_blank" rel="noopener">AlphaGo Zero</a></strong></span> (2017) &ndash; The latest improvement on AlphaGo, which showed an interesting random play/starting from scratch approach.</li>
</ul>
<h2><u>Other Important Papers</u></h2>
<p>I don't think I can do a deep learning overview blog post justice without finding some way to include these next research papers. Even though we can't point to unique use cases in industry, the following contributions have been critical in pushing state of the art deep learning.</p>
<ul>
<li><span style="text-decoration: underline;"><strong><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a></strong></span> (1998) &ndash; aka LeNet. Yann LeCun&rsquo;s successful use of CNNs applied to MNIST data. It wasn&rsquo;t until 2012 with AlexNet that CNNs started to do better on tougher image datasets, like ImageNet. Incredible that this paper is nearly 20 years old now!</li>
<li><a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>A Fast Learning Algorithm for DBNs</strong></span></a> (2006) &ndash; A Geoffrey Hinton paper that showed techniques for effectively training deep belief networks (as they were previously referred to).</li>
<li><span style="text-decoration: underline;"><strong><a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">Dropout</a></strong></span> (2013) - Hugely important regularization technique that drops out random neurons in DNNs to combat the classic issue of overfitting.</li>
<li><span style="text-decoration: underline;"><strong><a href="http://www.cs.toronto.edu/~fritz/absps/momentum.pdf" target="_blank" rel="noopener">On the Importance of Initialization and Momentum in DL</a></strong></span> (2013) &ndash; As the title of the paper suggests, the authors discuss SGD and the improvements that can be seen with careful weight initialization and proper momentum tuning.</li>
<li><span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">Adam: A Method for Stochastic Optimization</a></strong></span> (2014) &ndash; Adam is one of the most widely used optimization algorithms for training DNNs.</li>
<li><a href="https://arxiv.org/pdf/1411.1792.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>How Transferrable are Features in DNNs</strong></span></a> (2014) - First major study that shed light on the idea that features learned by filters in CNNs could be transferred to other networks and used as effective starting points.</li>
<li><a href="https://arxiv.org/pdf/1406.2661v1.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>Generative Adversarial Networks</strong></span></a> (2014) - The original GAN paper that introduced the use of discriminator and generator networks to model a data distribution.</li>
<li><a href="https://arxiv.org/pdf/1410.5401.pdf" target="_blank" rel="noopener"><span style="text-decoration: underline;"><strong>Neural Turing Machines</strong></span></a> (2014) &ndash; Examined the possible use of external memory coupled with standard DNNs. Work was expanded upon with the <span style="text-decoration: underline;"><strong><a href="https://deepmind.com/blog/differentiable-neural-computers/">Differentiable Neural Computer</a></strong></span> (2016).</li>
<li><span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization</a></strong></span> (2015) - Accelerated the training and stability of deep neural networks by addressing the problem of internal covariate shift.</li>
<li><span style="text-decoration: underline;"><strong><a href="https://arxiv.org/pdf/1508.06576.pdf" target="_blank" rel="noopener">Style Transfer</a></strong></span> (2015) - Showed how you can use deep neural nets to create artificial artistic images.</li>
</ul>
<h2><u>Going Forward </u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Technology is notoriously difficult to predict. Quite frankly, I think it's nearly impossible to predict what the technological landscape will look like 10 or more years into the future. However, when I first thought of making this post, I wanted to not only focus on the advancements over the last half decade, but also to spark a discussion on what type of impact deep learning can have in the future.</p>
<p>Like I mentioned in the Intro, I think deep learning is unique because we finally have ways of understanding speech, text, and images. This opens up so many different problem spaces in a wide variety of fields. Let&rsquo;s think of a couple.</p>
<ul>
<li>Based on aerial images, farmers can use CNNs to determine locations in their field where more soil or fertilizer is needed.</li>
<li>Doctors can use CNNs to help them detect patterns and find abnormalities in X-Rays and other imaging data.</li>
<li>Waste management companies can use CNNs to help sort through recycling and garbage debris.</li>
<li>Companies can use RNNs to create systems that help facilitate and direct conversations between customer service reps and users to the correct places.</li>
<li>Psychologists can use RNNs to help them detect changes or abnormalities in a person&rsquo;s speech patterns in order to spot symptoms of mental illness or depression.</li>
</ul>
<p>The capabilities that deep learning methods provide are not just available to the Big 5 or to tech startups exclusively in SF. Given the right amount of data, compute, and a clear end goal (*These are not trivial assumptions*), I think almost every organization/company/group in the world can make use of this technology.</p>
<p>And yes, this all seems great, but I do agree that for some problem spaces, <strong>deep learning is a square peg for a round hole</strong>. It&rsquo;s not the right solution sometimes. While the above are all reasonable application spaces, it&rsquo;s important for your company/organization to take time to figure out whether deep learning is the solution, or if a simple linear regression + data preprocessing workflow is a better option. For some types of data and for some problem spaces, traditional ML methods will be hugely effective and you should <em>definitely use them</em>.</p>
<p>But when it comes to the really interesting problems in our world today, more often than not they are going to be dealing with speech, text, or images. For those, deep learning is an extremely exciting option and I can&rsquo;t wait to see how the field evolves over the next 5 years.&nbsp;</p>
<h2><u>Conclusion</u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Just want to end with one more thought exercise. I was recently listening to an A16Z podcast episode called <a href="https://soundcloud.com/a16z/benedict-oreilly" target="_blank" rel="noopener">Platforming the Future</a>, where Tim O&rsquo;Reilly and Benedict Evans talked about recent waves of progress in tech. It made me wonder about how we&rsquo;re going to look at deep learning when we look back 20 years from now. Are we going to see it as a 5-10 year tech trend that slowly fizzled out or could it be the starting point on the quest to AGI, the greatest technological advancement in history?</p>
<p>Yes, deep learning is currently a buzz word. Yes, it is hyped. Yes, people use it in situations where they probably shouldn&rsquo;t. <strong>But</strong>, as you saw in this post, it is fueling incredible progress in today&rsquo;s tech world and it is solving real problems that we thought were impossible not too long ago. Seeing what we have accomplished over the last half decade and imagining the problems we&rsquo;ll solve and the lives we&rsquo;ll impact over the next half decade, well, that&rsquo;s just straight up exciting. <i class="em em-grinning"></i> &nbsp;</p>
<h2><u>Ways to Keep up With Deep Learning Progress</u></h2>
<ul>
<li><a href="https://news.ycombinator.com/" target="_blank" rel="noopener">HackerNews</a> - I <em>guarantee </em>you there is probably at least one deep learning/machine learning related news story within the top 60 at any given time (There's one at #18 at the time I'm writing this).</li>
<li><a href="https://www.reddit.com/r/MachineLearning/" target="_blank" rel="noopener">ML Subreddit</a> &ndash; Surprisingly, you can find a lot of in depth technical discussions here.</li>
<li><a href="https://twitter.com/" target="_blank" rel="noopener">Twitter</a> - Follow <a href="https://twitter.com/Smerity" target="_blank" rel="noopener">Smerity</a>, <a href="https://twitter.com/jackclarkSF" target="_blank" rel="noopener">Jack Clark</a>, <a href="https://twitter.com/karpathy" target="_blank" rel="noopener">Karpathy</a>, <a href="https://twitter.com/soumithchintala" target="_blank" rel="noopener">Soumith Chintala</a>, <a href="https://twitter.com/goodfellow_ian" target="_blank" rel="noopener">Ian Goodfellow</a>, <a href="https://twitter.com/hardmaru">hardmaru</a>, and <a href="https://twitter.com/aditdeshpande3" target="_blank" rel="noopener">I</a> also like to tweet/retweet about ML.</li>
<li><a href="https://jack-clark.net/" target="_blank" rel="noopener">Import AI Newsletter</a> &ndash; Jack Clark releases a weekly newsletter on artificial intelligence.</li>
<li><a href="https://www.facebook.com/groups/DeepNetGroup" target="_blank" rel="noopener">AI/DL Facebook Group</a> &ndash; Very active group where members post anything from news articles to blog posts to general ML questions.</li>
<li><a href="https://arxiv.org/list/stat.ML/recent" target="_blank" rel="noopener">Arxiv</a> &ndash; Definitely for more advanced practitioners, but searching through the new releases section is a great way to get a sense of where research in the field is headed.</li>
<li><a href="https://github.com/adeshpande3/Machine-Learning-Links-And-Lessons-Learned" target="_blank" rel="noopener">ML Github Repo</a> &ndash; I like to update this repo with any interesting papers/links/blogs I read about.</li>
</ul>
<p>Dueces. <i class="em em-v"></i></p>
<p><a href="/assets/Sources11.txt" target="_blank" rel="noopener"> Sources</a></p>
<p>&nbsp;</p>
<a href="https://twitter.com/share" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-80811190-1', 'auto');
  ga('send', 'pageview');
</script>
