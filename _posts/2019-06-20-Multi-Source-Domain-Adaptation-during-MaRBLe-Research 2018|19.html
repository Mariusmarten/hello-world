---
layout: post
excerpt: Maastricht University - Faculty of Psychology and Neuroscience (FPN)
permalink: /Multi-Source Domain Adaptation - MaRBLe Research 2018/19
published: true
images:
  - url: /assets/1_representation_network.png
---
<link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

<h2><u>Introduction </u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Humans are able to recognize scenes independently of the modality they perceive it in. In this paper, it is tested if scene specific features extracted from natural images are reusable for classifying clip art and sketches. Further, the ability of networks to hold multiple representations simultaneously is examined. To study this problem a pretrained convolutional neural network is used. Further, a randomly initialized classifier is added and retrained. The representations of clip art achieve the highest results, followed by sketches. Natural image performance remains notably below prior results. The experiment suggests that the overall transferability of learned features is not only limited by the distance but also by the diversity of the input distribution. Moreover, a multimodal representation is shown to be feasible but only poorly. Further research is needed to explain the obtained outcomes and to explain why they deviate from previous expectations.
  
<p><img src="/assets/1_Overview-size1.png" /></p>
<p>The figure shows a kitchen and an office (concepts) acrossdifferent modalities (clip art, sketches and natural images)..</p>

<h2><u>Can you recognize scenes across different styles? </u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The  input  variables  consist  of  natural  images  from  theScene205  dataset  collected  and  curated  by  B.  Zhou  et  al.(2014).   An  exhaustive  list  of  the  classes  can  be  found  inappendix 1. The set contains around 2.5 million images from205 scene categories.  The compressed file contains resized 256*256  images,  split  into  a  train  set  and  a  validation  setof Places 205 with a size of 126GB. Due to computationallimitations the number of pictures is reduced to around 80pictures  per  object,  so  that  16,400  pictures  remain.    Thesketch images consist of 14,830 training and 2,050 valida-tion  sketches  collected  through  Amazon  Mechanical  Turk,whereby different colors indicate different objects.  The clipart  data  includes  11,372  training  and  1,954  validation  clipart images downloaded from search engines.  Sketches andclip  art  were  assembled  by  Aytar  et  al.  (2018). </em></p>

<h2><u>VGG16 Architecture </u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The picture on the left shows the input of the network, consisting of an RGB image.  Thenetwork consists of packages of convolution layers followed by an activation function, namely a rectified linear unit (ReLU)and finally a maxpooling layer.  Those packages are repeated and followed by three fully connected layers.  Convolution andmaxpooling layers, shown in orange and red are frozen during training. The fully connected layers (here abbreviated with fc)make up the classifier, shown in purple.  Everything prior to the classifier is frozen and only the classifier is trained.</p>
<p><img src="/assets/1_representation_network.png" /></p>

<h2><u>Natural Language Processing</u></h2>
<p><img src="/assets/5Years6.png" /></p>

<h2><u>Conclusion</u></h2>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Just want to end with one more thought exercise. I was recently listening to an A16Z podcast episode called <a href="https://soundcloud.com/a16z/benedict-oreilly" target="_blank" rel="noopener">Platforming the Future</a>, where Tim O&rsquo;Reilly and Benedict Evans talked about recent waves of progress in tech. It made me wonder about how we&rsquo;re going to look at deep learning when we look back 20 years from now. Are we going to see it as a 5-10 year tech trend that slowly fizzled out or could it be the starting point on the quest to AGI, the greatest technological advancement in history?</p>
<p>Yes, deep learning is currently a buzz word. Yes, it is hyped. Yes, people use it in situations where they probably shouldn&rsquo;t. <strong>But</strong>, as you saw in this post, it is fueling incredible progress in today&rsquo;s tech world and it is solving real problems that we thought were impossible not too long ago. Seeing what we have accomplished over the last half decade and imagining the problems we&rsquo;ll solve and the lives we&rsquo;ll impact over the next half decade, well, that&rsquo;s just straight up exciting. <i class="em em-grinning"></i> &nbsp;</p>
<h2><u>Ways to Keep up With Deep Learning Progress</u></h2>
<ul>
<li><a href="https://news.ycombinator.com/" target="_blank" rel="noopener">HackerNews</a> - I <em>guarantee </em>you there is probably at least one deep learning/machine learning related news story within the top 60 at any given time (There's one at #18 at the time I'm writing this).</li>
<li><a href="https://www.reddit.com/r/MachineLearning/" target="_blank" rel="noopener">ML Subreddit</a> &ndash; Surprisingly, you can find a lot of in depth technical discussions here.</li>
<li><a href="https://twitter.com/" target="_blank" rel="noopener">Twitter</a> - Follow <a href="https://twitter.com/Smerity" target="_blank" rel="noopener">Smerity</a>, <a href="https://twitter.com/jackclarkSF" target="_blank" rel="noopener">Jack Clark</a>, <a href="https://twitter.com/karpathy" target="_blank" rel="noopener">Karpathy</a>, <a href="https://twitter.com/soumithchintala" target="_blank" rel="noopener">Soumith Chintala</a>, <a href="https://twitter.com/goodfellow_ian" target="_blank" rel="noopener">Ian Goodfellow</a>, <a href="https://twitter.com/hardmaru">hardmaru</a>, and <a href="https://twitter.com/aditdeshpande3" target="_blank" rel="noopener">I</a> also like to tweet/retweet about ML.</li>
<li><a href="https://jack-clark.net/" target="_blank" rel="noopener">Import AI Newsletter</a> &ndash; Jack Clark releases a weekly newsletter on artificial intelligence.</li>
<li><a href="https://www.facebook.com/groups/DeepNetGroup" target="_blank" rel="noopener">AI/DL Facebook Group</a> &ndash; Very active group where members post anything from news articles to blog posts to general ML questions.</li>
<li><a href="https://arxiv.org/list/stat.ML/recent" target="_blank" rel="noopener">Arxiv</a> &ndash; Definitely for more advanced practitioners, but searching through the new releases section is a great way to get a sense of where research in the field is headed.</li>
<li><a href="https://github.com/adeshpande3/Machine-Learning-Links-And-Lessons-Learned" target="_blank" rel="noopener">ML Github Repo</a> &ndash; I like to update this repo with any interesting papers/links/blogs I read about.</li>
</ul>
<p>Dueces. <i class="em em-v"></i></p>
<p><a href="/assets/Sources11.txt" target="_blank" rel="noopener"> Sources</a></p>
<p>&nbsp;</p>
<a href="https://twitter.com/share" class="twitter-share-button" data-show-count="false">Tweet</a><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-80811190-1', 'auto');
  ga('send', 'pageview');
</script>
